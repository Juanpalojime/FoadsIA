{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# üöÄ EnfoadsIA - Backend Production (T4 GPU) v2.0\n",
                "\n",
                "Este notebook ejecuta el ecosistema completo de **EnfoadsIA** en Google Colab con GPU T4.\n",
                "\n",
                "## üìã Instrucciones\n",
                "1. Aseg√∫rate de tener GPU habilitada: `Runtime > Change runtime type > T4 GPU`\n",
                "2. Ejecuta las celdas en orden\n",
                "3. Copia la URL p√∫blica generada para conectar tu frontend\n",
                "\n",
                "## ‚ú® Novedades v2.0\n",
                "- ‚úÖ Pre-descarga autom√°tica de modelos\n",
                "- ‚úÖ Face Swap con InsightFace\n",
                "- ‚úÖ Magic Prompt inteligente\n",
                "- ‚úÖ Monitoreo avanzado de GPU\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup",
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title üõ†Ô∏è 1. Configuraci√≥n del Entorno\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "\n",
                "# Configuraci√≥n\n",
                "REPO_URL = \"https://github.com/Juanpalojime/FoadsIA.git\"\n",
                "BASE_DIR = Path(\"/content\")\n",
                "REPO_DIR = BASE_DIR / \"FoadsIA\"\n",
                "BACKEND_DIR = REPO_DIR / \"backend\"\n",
                "\n",
                "def run_command(cmd, description, silent=False):\n",
                "    \"\"\"Ejecuta un comando con manejo de errores.\"\"\"\n",
                "    print(f\"‚öôÔ∏è  {description}...\")\n",
                "    try:\n",
                "        if silent:\n",
                "            result = subprocess.run(\n",
                "                cmd, \n",
                "                shell=True, \n",
                "                check=True,\n",
                "                stdout=subprocess.DEVNULL,\n",
                "                stderr=subprocess.DEVNULL\n",
                "            )\n",
                "        else:\n",
                "            result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True)\n",
                "        print(f\"‚úÖ {description} completado\")\n",
                "        return True\n",
                "    except subprocess.CalledProcessError as e:\n",
                "        print(f\"‚ùå Error en {description}: {e}\")\n",
                "        if not silent and hasattr(e, 'stderr'):\n",
                "            print(f\"   Detalles: {e.stderr[:200]}\")\n",
                "        return False\n",
                "\n",
                "# Resetear al directorio base\n",
                "os.chdir(BASE_DIR)\n",
                "print(f\"üìÅ Directorio base: {os.getcwd()}\\n\")\n",
                "\n",
                "# Clonar o actualizar repositorio\n",
                "print(\"üì° Configurando repositorio...\")\n",
                "if not REPO_DIR.exists():\n",
                "    if run_command(f\"git clone {REPO_URL}\", \"Clonando repositorio\"):\n",
                "        print(\"‚úÖ Repositorio clonado exitosamente\\n\")\n",
                "    else:\n",
                "        raise Exception(\"No se pudo clonar el repositorio\")\n",
                "else:\n",
                "    os.chdir(REPO_DIR)\n",
                "    if run_command(\"git pull\", \"Actualizando repositorio\"):\n",
                "        print(\"‚úÖ Repositorio actualizado\\n\")\n",
                "\n",
                "# Verificar estructura del proyecto\n",
                "if not BACKEND_DIR.exists():\n",
                "    raise Exception(f\"‚ùå No se encontr√≥ el directorio backend en {REPO_DIR}\")\n",
                "\n",
                "os.chdir(BACKEND_DIR)\n",
                "print(f\"‚úÖ Directorio de trabajo: {os.getcwd()}\\n\")\n",
                "\n",
                "# Instalar dependencias del sistema\n",
                "print(\"üì¶ Instalando dependencias del sistema...\")\n",
                "system_packages = \"ffmpeg libsm6 libxext6 libgl1\"\n",
                "run_command(\n",
                "    f\"apt-get update -qq && apt-get install -y -qq {system_packages}\",\n",
                "    \"Dependencias del sistema\",\n",
                "    silent=True\n",
                ")\n",
                "\n",
                "# Instalar dependencias de Python\n",
                "print(\"\\nüêç Instalando dependencias de Python...\")\n",
                "if (BACKEND_DIR / \"requirements.txt\").exists():\n",
                "    run_command(\n",
                "        \"pip install -q -r requirements.txt\",\n",
                "        \"Instalaci√≥n de requirements.txt\",\n",
                "        silent=True\n",
                "    )\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  No se encontr√≥ requirements.txt\")\n",
                "\n",
                "# Asegurar dependencias cr√≠ticas\n",
                "print(\"\\nüîß Verificando dependencias cr√≠ticas...\")\n",
                "critical_packages = \"flask flask-socketio flask-cors eventlet pyngrok torch torchvision\"\n",
                "run_command(\n",
                "    f\"pip install -q {critical_packages}\",\n",
                "    \"Dependencias cr√≠ticas\",\n",
                "    silent=True\n",
                ")\n",
                "\n",
                "# Verificar GPU\n",
                "print(\"\\nüéÆ Verificando disponibilidad de GPU...\")\n",
                "try:\n",
                "    import torch\n",
                "    if torch.cuda.is_available():\n",
                "        gpu_name = torch.cuda.get_device_name(0)\n",
                "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "        print(f\"‚úÖ GPU DETECTADA: {gpu_name}\")\n",
                "        print(f\"   Memoria: {gpu_memory:.1f} GB\")\n",
                "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è  GPU no disponible - el modelo correr√° en CPU (muy lento)\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error al verificar GPU: {e}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üéâ CONFIGURACI√ìN COMPLETADA - Listo para pre-descarga\")\n",
                "print(\"=\"*60 + \"\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "preload",
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title üì• 1.5. Pre-descarga de Modelos (Opcional pero Recomendado)\n",
                "\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "print(\"üì• PRE-DESCARGA DE MODELOS DE IA\\n\")\n",
                "print(\"=\"*70)\n",
                "print(\"‚è±Ô∏è  Esto puede tomar 5-10 minutos la primera vez\")\n",
                "print(\"üí° Los modelos se cachean, las siguientes ejecuciones ser√°n instant√°neas\")\n",
                "print(\"=\"*70 + \"\\n\")\n",
                "\n",
                "# Cambiar al directorio backend\n",
                "BACKEND_DIR = Path(\"/content/FoadsIA/backend\")\n",
                "if BACKEND_DIR.exists():\n",
                "    os.chdir(BACKEND_DIR)\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  Ejecuta primero la celda de configuraci√≥n\")\n",
                "    raise Exception(\"Backend directory not found\")\n",
                "\n",
                "# Ejecutar script de pre-descarga\n",
                "if (BACKEND_DIR / \"preload_models.py\").exists():\n",
                "    !python preload_models.py\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  Script preload_models.py no encontrado\")\n",
                "    print(\"üí° Descargando modelos manualmente...\\n\")\n",
                "    \n",
                "    # Fallback: descargar modelos manualmente\n",
                "    from huggingface_hub import hf_hub_download\n",
                "    import torch\n",
                "    \n",
                "    # SDXL Lightning\n",
                "    print(\"üé® Descargando SDXL Lightning...\")\n",
                "    try:\n",
                "        repo = \"ByteDance/SDXL-Lightning\"\n",
                "        ckpt = \"sdxl_lightning_4step_unet.safetensors\"\n",
                "        model_path = hf_hub_download(repo, ckpt)\n",
                "        print(f\"‚úÖ SDXL Lightning descargado\\n\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error: {e}\\n\")\n",
                "    \n",
                "    # Whisper\n",
                "    print(\"üé§ Descargando Faster-Whisper...\")\n",
                "    try:\n",
                "        from faster_whisper import WhisperModel\n",
                "        model = WhisperModel(\"base\", device=\"cpu\", compute_type=\"float16\")\n",
                "        print(\"‚úÖ Whisper descargado\\n\")\n",
                "        del model\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error: {e}\\n\")\n",
                "    \n",
                "    # InsightFace\n",
                "    print(\"üë§ Descargando InsightFace...\")\n",
                "    try:\n",
                "        import insightface\n",
                "        from insightface.app import FaceAnalysis\n",
                "        app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])\n",
                "        app.prepare(ctx_id=-1, det_size=(640, 640))\n",
                "        print(\"‚úÖ InsightFace descargado\\n\")\n",
                "        del app\n",
                "    except Exception as e:\n",
                "        print(f\"‚ùå Error: {e}\\n\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*70)\n",
                "print(\"üéâ PRE-DESCARGA COMPLETADA\")\n",
                "print(\"=\"*70)\n",
                "print(\"\\nüí° Ahora puedes ejecutar el servidor sin esperas de descarga\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "run",
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title üöÄ 2. Ejecutar Servidor Backend\n",
                "\n",
                "import os\n",
                "import time\n",
                "import threading\n",
                "from pathlib import Path\n",
                "from pyngrok import ngrok\n",
                "\n",
                "# Token de ngrok (obt√©n el tuyo en https://dashboard.ngrok.com)\n",
                "AUTH_TOKEN = \"2yHQiBeYhFdbJSiK31054jtsKkw_54yvtD5Cs9mK2yhFgQ2j\" #@param {type:\"string\"}\n",
                "PORT = 5000 #@param {type:\"integer\"}\n",
                "\n",
                "# Validar directorio\n",
                "BACKEND_DIR = Path(\"/content/FoadsIA/backend\")\n",
                "if not BACKEND_DIR.exists():\n",
                "    raise Exception(f\"‚ùå Directorio backend no encontrado: {BACKEND_DIR}\")\n",
                "\n",
                "os.chdir(BACKEND_DIR)\n",
                "print(f\"üìÅ Directorio actual: {os.getcwd()}\\n\")\n",
                "\n",
                "# Verificar app.py\n",
                "if not (BACKEND_DIR / \"app.py\").exists():\n",
                "    raise Exception(\"‚ùå No se encontr√≥ app.py en el directorio backend\")\n",
                "\n",
                "# Configurar ngrok\n",
                "if AUTH_TOKEN:\n",
                "    try:\n",
                "        print(\"üîê Configurando autenticaci√≥n de ngrok...\")\n",
                "        ngrok.set_auth_token(AUTH_TOKEN)\n",
                "        print(\"‚úÖ Token configurado\\n\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è  Error al configurar token: {e}\\n\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  No se proporcion√≥ token de ngrok - la sesi√≥n ser√° limitada\\n\")\n",
                "\n",
                "# Limpiar t√∫neles existentes\n",
                "print(\"üßπ Limpiando t√∫neles anteriores...\")\n",
                "try:\n",
                "    ngrok.kill()\n",
                "    time.sleep(1)\n",
                "    print(\"‚úÖ T√∫neles limpiados\\n\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ÑπÔ∏è  No hab√≠a t√∫neles previos\\n\")\n",
                "\n",
                "# Crear t√∫nel\n",
                "print(f\"üåê Creando t√∫nel p√∫blico en puerto {PORT}...\")\n",
                "try:\n",
                "    tunnel = ngrok.connect(PORT, bind_tls=True)\n",
                "    public_url = tunnel.public_url\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*70)\n",
                "    print(\"üéâ SERVIDOR P√öBLICO ACTIVO\")\n",
                "    print(\"=\"*70)\n",
                "    print(f\"\\nüì° URL del Backend: {public_url}\")\n",
                "    print(f\"\\nüí° Usa esta URL en tu frontend (Settings) para conectarte al servidor\")\n",
                "    print(f\"\\n‚ö†Ô∏è  IMPORTANTE: Este t√∫nel se cerrar√° si detienes esta celda\")\n",
                "    print(\"=\"*70 + \"\\n\")\n",
                "    \n",
                "    # Mostrar t√∫neles activos\n",
                "    tunnels = ngrok.get_tunnels()\n",
                "    if tunnels:\n",
                "        print(\"üìã T√∫neles activos:\")\n",
                "        for t in tunnels:\n",
                "            print(f\"   ‚Ä¢ {t.public_url} -> {t.config['addr']}\")\n",
                "        print()\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error al crear t√∫nel: {e}\")\n",
                "    print(\"\\nüí° Soluciones posibles:\")\n",
                "    print(\"   1. Verifica tu token de ngrok\")\n",
                "    print(\"   2. Revisa tu conexi√≥n a internet\")\n",
                "    print(\"   3. Intenta ejecutar la celda nuevamente\")\n",
                "    raise\n",
                "\n",
                "# Iniciar servidor Flask\n",
                "print(\"üöÄ Iniciando servidor Flask...\\n\")\n",
                "print(\"=\"*70)\n",
                "print(\"üìù LOGS DEL SERVIDOR (Ctrl+C para detener):\")\n",
                "print(\"=\"*70 + \"\\n\")\n",
                "\n",
                "try:\n",
                "    # Ejecutar app.py\n",
                "    !python app.py\n",
                "    \n",
                "except KeyboardInterrupt:\n",
                "    print(\"\\n\\n‚ö†Ô∏è  Servidor detenido por el usuario\")\n",
                "    ngrok.kill()\n",
                "    print(\"‚úÖ T√∫neles cerrados\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"\\n\\n‚ùå Error al ejecutar servidor: {e}\")\n",
                "    ngrok.kill()\n",
                "    raise\n",
                "\n",
                "finally:\n",
                "    print(\"\\nüîö Limpieza completada\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "debug",
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title üîç 3. Diagn√≥stico y Pruebas (Opcional)\n",
                "\n",
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "print(\"üîç DIAGN√ìSTICO DEL SISTEMA\\n\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Informaci√≥n del sistema\n",
                "print(\"\\nüìä Sistema Operativo:\")\n",
                "!uname -a\n",
                "\n",
                "print(\"\\nüêç Versi√≥n de Python:\")\n",
                "print(f\"   {sys.version}\")\n",
                "\n",
                "print(\"\\nüì¶ Paquetes principales instalados:\")\n",
                "packages = [\"torch\", \"flask\", \"flask-socketio\", \"pyngrok\", \"opencv-python\", \"diffusers\", \"insightface\"]\n",
                "for pkg in packages:\n",
                "    try:\n",
                "        result = !pip show {pkg} | grep Version\n",
                "        if result:\n",
                "            print(f\"   ‚úÖ {pkg}: {result[0].split(':')[1].strip()}\")\n",
                "        else:\n",
                "            print(f\"   ‚ùå {pkg}: No instalado\")\n",
                "    except:\n",
                "        print(f\"   ‚ùå {pkg}: Error al verificar\")\n",
                "\n",
                "print(\"\\nüéÆ Estado de GPU:\")\n",
                "try:\n",
                "    import torch\n",
                "    print(f\"   GPU disponible: {torch.cuda.is_available()}\")\n",
                "    if torch.cuda.is_available():\n",
                "        print(f\"   Nombre: {torch.cuda.get_device_name(0)}\")\n",
                "        print(f\"   Memoria total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "        print(f\"   Memoria libre: {torch.cuda.memory_reserved(0) / 1e9:.1f} GB\")\n",
                "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
                "except Exception as e:\n",
                "    print(f\"   ‚ùå Error: {e}\")\n",
                "\n",
                "print(\"\\nüìÅ Estructura del proyecto:\")\n",
                "backend_dir = Path(\"/content/FoadsIA/backend\")\n",
                "if backend_dir.exists():\n",
                "    print(f\"   üìÇ {backend_dir}\")\n",
                "    for item in sorted(backend_dir.iterdir())[:15]:\n",
                "        icon = \"üìÅ\" if item.is_dir() else \"üìÑ\"\n",
                "        size = f\"({item.stat().st_size / 1024:.1f} KB)\" if item.is_file() else \"\"\n",
                "        print(f\"      {icon} {item.name} {size}\")\n",
                "    if len(list(backend_dir.iterdir())) > 15:\n",
                "        print(f\"      ... y {len(list(backend_dir.iterdir())) - 15} m√°s\")\n",
                "else:\n",
                "    print(\"   ‚ùå Directorio backend no encontrado\")\n",
                "\n",
                "print(\"\\nüíæ Espacio en disco:\")\n",
                "try:\n",
                "    import shutil\n",
                "    total, used, free = shutil.disk_usage(\"/\")\n",
                "    print(f\"   Total: {total // (2**30)} GB\")\n",
                "    print(f\"   Usado: {used // (2**30)} GB\")\n",
                "    print(f\"   Libre: {free // (2**30)} GB\")\n",
                "except:\n",
                "    pass\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ Diagn√≥stico completado\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "footer"
            },
            "source": [
                "---\n",
                "\n",
                "## üìö Notas Importantes\n",
                "\n",
                "### ‚ö° Uso de GPU\n",
                "- Aseg√∫rate de seleccionar **GPU T4** en `Runtime > Change runtime type`\n",
                "- La sesi√≥n de GPU es limitada en Colab gratuito (~12 horas)\n",
                "- El sistema optimiza VRAM autom√°ticamente con offloading\n",
                "\n",
                "### üåê Ngrok\n",
                "- Obt√©n tu token gratuito en [ngrok.com](https://dashboard.ngrok.com/get-started/your-authtoken)\n",
                "- Sin token, las sesiones duran m√°ximo 2 horas\n",
                "- Con token, obtienes sesiones m√°s largas y URLs personalizadas\n",
                "\n",
                "### üîí Seguridad\n",
                "- **NO compartas** tu token de ngrok p√∫blicamente\n",
                "- La URL p√∫blica es temporal y expira cuando detienes el servidor\n",
                "- Considera implementar autenticaci√≥n en producci√≥n\n",
                "\n",
                "### ‚ú® Nuevas Caracter√≠sticas v2.0\n",
                "- **Magic Prompt**: Mejora autom√°tica de prompts\n",
                "- **Face Swap**: Intercambio de rostros con InsightFace\n",
                "- **GPU Monitoring**: Monitoreo detallado de VRAM\n",
                "- **Pre-descarga**: Sistema autom√°tico de cach√© de modelos\n",
                "\n",
                "### üí° Troubleshooting\n",
                "- Si el servidor no inicia, ejecuta la celda de diagn√≥stico\n",
                "- Verifica que `app.py` exista en el repositorio\n",
                "- Revisa los logs para identificar errores espec√≠ficos\n",
                "- Si hay problemas de VRAM, el sistema offload es autom√°tico\n",
                "\n",
                "### üìä Endpoints Disponibles\n",
                "- `GET /` - Health check\n",
                "- `GET /gpu-status` - Estado de GPU y VRAM\n",
                "- `POST /magic-prompt` - Mejora de prompts\n",
                "- `POST /generate-image` - Generaci√≥n con SDXL Lightning\n",
                "- `POST /face-swap` - Intercambio de rostros\n",
                "- `POST /render-video` - Videos con avatares\n",
                "- `POST /render-multi-scene` - Videos multi-escena\n",
                "\n",
                "---\n",
                "\n",
                "**Desarrollado por EnfoadsIA Team** üöÄ  \n",
                "**Versi√≥n**: 2.0  \n",
                "**√öltima actualizaci√≥n**: 2025-12-27"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}