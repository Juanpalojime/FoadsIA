{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# ðŸš€ FoadsIA - Backend Production (T4 GPU)\n",
                "\n",
                "Este notebook ejecuta el ecosistema completo de **FoadsIA** en Google Colab con GPU T4.\n",
                "\n",
                "## ðŸ“‹ Instrucciones\n",
                "1. AsegÃºrate de tener GPU habilitada: `Runtime > Change runtime type > T4 GPU`\n",
                "2. Ejecuta las celdas en orden\n",
                "3. Copia la URL pÃºblica generada para conectar tu frontend\n",
                "\n",
                "## âš¡ Ãšltima ActualizaciÃ³n\n",
                "- âœ… IntegraciÃ³n con **Master Loader Script**\n",
                "- âœ… Descarga y VerificaciÃ³n AutomÃ¡tica de Modelos\n",
                "- âœ… Soporte SDXL Lightning (Hiperrealismo)\n",
                "- âœ… Soporte Whisper (SubtÃ­tulos)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup",
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title ðŸ› ï¸ 1. ConfiguraciÃ³n del Entorno\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "\n",
                "# ConfiguraciÃ³n\n",
                "REPO_URL = \"https://github.com/Juanpalojime/FoadsIA.git\"\n",
                "BASE_DIR = Path(\"/content\")\n",
                "REPO_DIR = BASE_DIR / \"FoadsIA\"\n",
                "BACKEND_DIR = REPO_DIR / \"backend\"\n",
                "\n",
                "def run_command(cmd, description, silent=False):\n",
                "    \"\"\"Ejecuta un comando con manejo de errores.\"\"\"\n",
                "    print(f\"âš™ï¸  {description}...\")\n",
                "    try:\n",
                "        if silent:\n",
                "            result = subprocess.run(\n",
                "                cmd, \n",
                "                shell=True, \n",
                "                check=True,\n",
                "                stdout=subprocess.DEVNULL,\n",
                "                stderr=subprocess.DEVNULL\n",
                "            )\n",
                "        else:\n",
                "            result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True)\n",
                "        print(f\"âœ… {description} completado\")\n",
                "        return True\n",
                "    except subprocess.CalledProcessError as e:\n",
                "        print(f\"âŒ Error en {description}: {e}\")\n",
                "        if not silent and hasattr(e, 'stderr'):\n",
                "            print(f\"   Detalles: {e.stderr[:200]}\")\n",
                "        return False\n",
                "\n",
                "# Resetear al directorio base\n",
                "os.chdir(BASE_DIR)\n",
                "print(f\"ðŸ“ Directorio base: {os.getcwd()}\\n\")\n",
                "\n",
                "# Clonar o actualizar repositorio\n",
                "print(\"ðŸ“¡ Configurando repositorio...\")\n",
                "if not REPO_DIR.exists():\n",
                "    if run_command(f\"git clone {REPO_URL}\", \"Clonando repositorio\"):\n",
                "        print(\"âœ… Repositorio clonado exitosamente\\n\")\n",
                "    else:\n",
                "        raise Exception(\"No se pudo clonar el repositorio\")\n",
                "else:\n",
                "    os.chdir(REPO_DIR)\n",
                "    if run_command(\"git pull origin master\", \"Actualizando repositorio\"):\n",
                "        print(\"âœ… Repositorio actualizado\\n\")\n",
                "\n",
                "# Verificar estructura del proyecto\n",
                "if not BACKEND_DIR.exists():\n",
                "    raise Exception(f\"âŒ No se encontrÃ³ el directorio backend en {REPO_DIR}\")\n",
                "\n",
                "os.chdir(BACKEND_DIR)\n",
                "print(f\"âœ… Directorio de trabajo: {os.getcwd()}\\n\")\n",
                "\n",
                "# Instalar dependencias del sistema\n",
                "print(\"ðŸ“¦ Instalando dependencias del sistema...\")\n",
                "system_packages = \"ffmpeg libsm6 libxext6 libgl1\"\n",
                "run_command(\n",
                "    f\"apt-get update -qq && apt-get install -y -qq {system_packages}\",\n",
                "    \"Dependencias del sistema\",\n",
                "    silent=True\n",
                ")\n",
                "\n",
                "# Instalar dependencias de Python\n",
                "print(\"\\nðŸ Instalando dependencias de Python...\")\n",
                "if (BACKEND_DIR / \"requirements.txt\").exists():\n",
                "    run_command(\n",
                "        \"pip install -q -r requirements.txt\",\n",
                "        \"InstalaciÃ³n de requirements.txt\",\n",
                "        silent=True\n",
                "    )\n",
                "else:\n",
                "    print(\"âš ï¸  No se encontrÃ³ requirements.txt\")\n",
                "\n",
                "# Asegurar dependencias crÃ­ticas\n",
                "print(\"\\nðŸ”§ Verificando dependencias crÃ­ticas...\")\n",
                "critical_packages = \"flask flask-socketio flask-cors eventlet pyngrok torch torchvision diffusers huggingface_hub openai-whisper edge-tts\"\n",
                "run_command(\n",
                "    f\"pip install -q {critical_packages}\",\n",
                "    \"Dependencias crÃ­ticas\",\n",
                "    silent=True\n",
                ")\n",
                "\n",
                "# Verificar GPU\n",
                "print(\"\\nðŸŽ® Verificando disponibilidad de GPU...\")\n",
                "try:\n",
                "    import torch\n",
                "    if torch.cuda.is_available():\n",
                "        gpu_name = torch.cuda.get_device_name(0)\n",
                "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "        print(f\"âœ… GPU DETECTADA: {gpu_name}\")\n",
                "        print(f\"   Memoria: {gpu_memory:.1f} GB\")\n",
                "    else:\n",
                "        print(\"âš ï¸  GPU no disponible - el modelo correrÃ¡ en CPU (muy lento)\")\n",
                "except Exception as e:\n",
                "    print(f\"âŒ Error al verificar GPU: {e}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"ðŸŽ‰ CONFIGURACIÃ“N COMPLETADA - Listo para preparar modelos\")\n",
                "print(\"=\"*60 + \"\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "prepare_models",
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title â¬‡ï¸ 1.5 Preparar Modelos (Master Loader)\n",
                "\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Validar directorio\n",
                "BACKEND_DIR = Path(\"/content/FoadsIA/backend\")\n",
                "os.chdir(BACKEND_DIR)\n",
                "\n",
                "print(\"ðŸ¤– EJECUTANDO SCRIPT MAESTRO DE MODELOS...\\n\")\n",
                "if (BACKEND_DIR / \"master_loader.py\").exists():\n",
                "    !python master_loader.py\n",
                "else:\n",
                "    print(\"âŒ No se encontrÃ³ master_loader.py. Por favor actualiza el repositorio (paso 1).\")\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "run",
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title ðŸš€ 2. Ejecutar Servidor Backend\n",
                "\n",
                "import os\n",
                "import time\n",
                "import threading\n",
                "from pathlib import Path\n",
                "from pyngrok import ngrok\n",
                "\n",
                "# Token de ngrok (obtÃ©n el tuyo en https://dashboard.ngrok.com)\n",
                "AUTH_TOKEN = \"2yHQiBeYhFdbJSiK31054jtsKkw_54yvtD5Cs9mK2yhFgQ2j\" #@param {type:\"string\"}\n",
                "PORT = 5000 #@param {type:\"integer\"}\n",
                "\n",
                "# Validar directorio\n",
                "BACKEND_DIR = Path(\"/content/FoadsIA/backend\")\n",
                "os.chdir(BACKEND_DIR)\n",
                "\n",
                "# Configurar ngrok\n",
                "if AUTH_TOKEN:\n",
                "    try:\n",
                "        ngrok.set_auth_token(AUTH_TOKEN)\n",
                "        print(\"âœ… Token de Ngrok configurado\")\n",
                "    except Exception as e:\n",
                "        print(f\"âš ï¸  Error al configurar token: {e}\\n\")\n",
                "else:\n",
                "    print(\"âš ï¸  No se proporcionÃ³ token de ngrok\\n\")\n",
                "\n",
                "# Limpiar tÃºneles existentes\n",
                "try:\n",
                "    ngrok.kill()\n",
                "    time.sleep(1)\n",
                "except: pass\n",
                "\n",
                "# Crear tÃºnel\n",
                "print(f\"ðŸŒ Iniciando tÃºnel pÃºblico en puerto {PORT}...\")\n",
                "try:\n",
                "    tunnel = ngrok.connect(PORT, bind_tls=True)\n",
                "    public_url = tunnel.public_url\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*70)\n",
                "    print(\"ðŸŽ‰ SERVIDOR PÃšBLICO ACTIVO\")\n",
                "    print(\"=\"*70)\n",
                "    print(f\"\\nðŸ“¡ URL del Backend: {public_url}\")\n",
                "    print(f\"\\nâš ï¸  IMPORTANTE: No cierres esta pestaÃ±a de navegador\")\n",
                "    print(\"=\"*70 + \"\\n\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"âŒ Error al crear tÃºnel: {e}\")\n",
                "    raise\n",
                "\n",
                "# Iniciar servidor Flask\n",
                "print(\"ðŸš€ Iniciando servidor Flask...\\n\")\n",
                "try:\n",
                "    !python app.py\n",
                "except KeyboardInterrupt:\n",
                "    print(\"\\nðŸ›‘ Servidor detenido.\")\n",
                "    ngrok.kill()\n",
                "except Exception as e:\n",
                "    print(f\"\\nâŒ Error crÃ­tico: {e}\")\n",
                "    ngrok.kill()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "footer"
            },
            "source": [
                "---\n",
                "\n",
                "## ðŸ”§ Troubleshooting\n",
                "\n",
                "- Si falla la descarga de modelos, simplemente **vuelve a ejecutar la celda 1.5**.\n",
                "- Si el servidor se detiene inesperadamente, revisa si la GPU sigua asignada en `Runtime > View resources`.\n",
                "\n",
                "**Desarrollado por FoadsIA Team** ðŸš€"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}