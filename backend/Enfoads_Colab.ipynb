{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# üöÄ FoadsIA - Backend Production (T4 GPU)\n",
                "\n",
                "Este notebook ejecuta el ecosistema completo de **FoadsIA** en Google Colab con GPU T4.\n",
                "\n",
                "## üìã Instrucciones\n",
                "1. Aseg√∫rate de tener GPU habilitada: `Runtime > Change runtime type > T4 GPU`\n",
                "2. Ejecuta las celdas en orden\n",
                "3. Copia la URL p√∫blica generada para conectar tu frontend\n",
                "\n",
                "## ‚ö° √öltima Actualizaci√≥n\n",
                "- ‚úÖ Compatibilidad con Diffusers v1.0.0+\n",
                "- ‚úÖ Sistema de cach√© de im√°genes\n",
                "- ‚úÖ Descarga expl√≠cita de modelos\n",
                "- ‚úÖ Manejo graceful de errores\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "setup",
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title üõ†Ô∏è 1. Configuraci√≥n del Entorno\n",
                "\n",
                "import os\n",
                "import sys\n",
                "import subprocess\n",
                "from pathlib import Path\n",
                "\n",
                "# Configuraci√≥n\n",
                "REPO_URL = \"https://github.com/Juanpalojime/FoadsIA.git\"\n",
                "BASE_DIR = Path(\"/content\")\n",
                "REPO_DIR = BASE_DIR / \"FoadsIA\"\n",
                "BACKEND_DIR = REPO_DIR / \"backend\"\n",
                "\n",
                "def run_command(cmd, description, silent=False):\n",
                "    \"\"\"Ejecuta un comando con manejo de errores.\"\"\"\n",
                "    print(f\"‚öôÔ∏è  {description}...\")\n",
                "    try:\n",
                "        if silent:\n",
                "            result = subprocess.run(\n",
                "                cmd, \n",
                "                shell=True, \n",
                "                check=True,\n",
                "                stdout=subprocess.DEVNULL,\n",
                "                stderr=subprocess.DEVNULL\n",
                "            )\n",
                "        else:\n",
                "            result = subprocess.run(cmd, shell=True, check=True, capture_output=True, text=True)\n",
                "        print(f\"‚úÖ {description} completado\")\n",
                "        return True\n",
                "    except subprocess.CalledProcessError as e:\n",
                "        print(f\"‚ùå Error en {description}: {e}\")\n",
                "        if not silent and hasattr(e, 'stderr'):\n",
                "            print(f\"   Detalles: {e.stderr[:200]}\")\n",
                "        return False\n",
                "\n",
                "# Resetear al directorio base\n",
                "os.chdir(BASE_DIR)\n",
                "print(f\"üìÅ Directorio base: {os.getcwd()}\\n\")\n",
                "\n",
                "# Clonar o actualizar repositorio\n",
                "print(\"üì° Configurando repositorio...\")\n",
                "if not REPO_DIR.exists():\n",
                "    if run_command(f\"git clone {REPO_URL}\", \"Clonando repositorio\"):\n",
                "        print(\"‚úÖ Repositorio clonado exitosamente\\n\")\n",
                "    else:\n",
                "        raise Exception(\"No se pudo clonar el repositorio\")\n",
                "else:\n",
                "    os.chdir(REPO_DIR)\n",
                "    if run_command(\"git pull origin master\", \"Actualizando repositorio\"):\n",
                "        print(\"‚úÖ Repositorio actualizado\\n\")\n",
                "\n",
                "# Verificar estructura del proyecto\n",
                "if not BACKEND_DIR.exists():\n",
                "    raise Exception(f\"‚ùå No se encontr√≥ el directorio backend en {REPO_DIR}\")\n",
                "\n",
                "os.chdir(BACKEND_DIR)\n",
                "print(f\"‚úÖ Directorio de trabajo: {os.getcwd()}\\n\")\n",
                "\n",
                "# Instalar dependencias del sistema\n",
                "print(\"üì¶ Instalando dependencias del sistema...\")\n",
                "system_packages = \"ffmpeg libsm6 libxext6 libgl1\"\n",
                "run_command(\n",
                "    f\"apt-get update -qq && apt-get install -y -qq {system_packages}\",\n",
                "    \"Dependencias del sistema\",\n",
                "    silent=True\n",
                ")\n",
                "\n",
                "# Instalar dependencias de Python\n",
                "print(\"\\nüêç Instalando dependencias de Python...\")\n",
                "if (BACKEND_DIR / \"requirements.txt\").exists():\n",
                "    run_command(\n",
                "        \"pip install -q -r requirements.txt\",\n",
                "        \"Instalaci√≥n de requirements.txt\",\n",
                "        silent=True\n",
                "    )\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  No se encontr√≥ requirements.txt\")\n",
                "\n",
                "# Asegurar dependencias cr√≠ticas\n",
                "print(\"\\nüîß Verificando dependencias cr√≠ticas...\")\n",
                "critical_packages = \"flask flask-socketio flask-cors eventlet pyngrok torch torchvision diffusers huggingface_hub openai-whisper\"\n",
                "run_command(\n",
                "    f\"pip install -q {critical_packages}\",\n",
                "    \"Dependencias cr√≠ticas\",\n",
                "    silent=True\n",
                ")\n",
                "\n",
                "# Verificar versi√≥n de diffusers\n",
                "print(\"\\nüì¶ Verificando versi√≥n de diffusers...\")\n",
                "try:\n",
                "    import diffusers\n",
                "    print(f\"‚úÖ Diffusers version: {diffusers.__version__}\")\n",
                "except:\n",
                "    print(\"‚ö†Ô∏è  Diffusers no instalado correctamente\")\n",
                "\n",
                "# Verificar GPU\n",
                "print(\"\\nüéÆ Verificando disponibilidad de GPU...\")\n",
                "try:\n",
                "    import torch\n",
                "    if torch.cuda.is_available():\n",
                "        gpu_name = torch.cuda.get_device_name(0)\n",
                "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
                "        print(f\"‚úÖ GPU DETECTADA: {gpu_name}\")\n",
                "        print(f\"   Memoria: {gpu_memory:.1f} GB\")\n",
                "        print(f\"   CUDA Version: {torch.version.cuda}\")\n",
                "    else:\n",
                "        print(\"‚ö†Ô∏è  GPU no disponible - el modelo correr√° en CPU (muy lento)\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error al verificar GPU: {e}\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üéâ CONFIGURACI√ìN COMPLETADA - Listo para descargar modelos\")\n",
                "print(\"=\"*60 + \"\\n\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download_models",
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title ‚¨áÔ∏è 1.2 Descargar Modelos (Importante)\n",
                "\n",
                "import torch\n",
                "from huggingface_hub import hf_hub_download\n",
                "from diffusers import DiffusionPipeline\n",
                "\n",
                "print(\"‚¨áÔ∏è DESCARGANDO MODELOS\\n\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "def download_sdxl_lightning():\n",
                "    print(\"\\n‚ö° SDXL Lightning (ByteDance/SDXL-Lightning)...\")\n",
                "    print(\"   Este modelo es GRANDE (5.14 GB). Por favor espera...\")\n",
                "    \n",
                "    base = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
                "    repo = \"ByteDance/SDXL-Lightning\"\n",
                "    ckpt = \"sdxl_lightning_4step_unet.safetensors\"\n",
                "    \n",
                "    try:\n",
                "        # Descargar UNet checkpoint expl√≠citamente\n",
                "        print(\"   ‚è≥ Descargando UNet checkpoint...\")\n",
                "        file_path = hf_hub_download(repo, ckpt)\n",
                "        print(f\"   ‚úÖ Checkpoint descargado en: {file_path}\")\n",
                "        \n",
                "        # Descargar componentes base (VAE, Tokenizer)\n",
                "        # Esto asegura que when carguemos el pipeline 'base', ya est√© en cach√©\n",
                "        print(\"   ‚è≥ Descargando componentes base (si no existen)...\")\n",
                "        DiffusionPipeline.from_pretrained(\n",
                "            base, \n",
                "            torch_dtype=torch.float16, \n",
                "            variant=\"fp16\", \n",
                "            use_safetensors=True\n",
                "        )\n",
                "        print(\"   ‚úÖ SDXL Lightning listo para usar\")\n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ùå Error al descargar SDXL: {e}\")\n",
                "\n",
                "def download_whisper():\n",
                "    print(\"\\nüéôÔ∏è Whisper (openai/whisper-small)...\")\n",
                "    try:\n",
                "        import whisper\n",
                "        whisper.load_model(\"small\")\n",
                "        print(\"   ‚úÖ Whisper descargado correctamente\")\n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ùå Error al descargar Whisper: {e}\")\n",
                "        \n",
                "download_sdxl_lightning()\n",
                "download_whisper()\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"üéâ DESCARGAS COMPLETADAS - Listo para ejecutar servidor\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "clean_cache",
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title üßπ 1.5 Limpiar Cach√© (Ejecutar SOLO si hay errores)\n",
                "\n",
                "import os\n",
                "import shutil\n",
                "from pathlib import Path\n",
                "\n",
                "print(\"üßπ LIMPIEZA DE CACH√â\\n\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Limpiar cach√© de Hugging Face\n",
                "hf_cache = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
                "if hf_cache.exists():\n",
                "    print(\"\\nüì¶ Limpiando cach√© de Hugging Face...\")\n",
                "    sdxl_cache = hf_cache / \"models--ByteDance--SDXL-Lightning\"\n",
                "    if sdxl_cache.exists():\n",
                "        shutil.rmtree(sdxl_cache)\n",
                "        print(\"‚úÖ Cach√© de SDXL Lightning eliminado\")\n",
                "    else:\n",
                "        print(\"‚ÑπÔ∏è  No hay cach√© de SDXL Lightning\")\n",
                "else:\n",
                "    print(\"‚ÑπÔ∏è  No hay cach√© de Hugging Face\")\n",
                "\n",
                "# Limpiar cach√© de torch\n",
                "torch_cache = Path.home() / \".cache\" / \"torch\"\n",
                "if torch_cache.exists():\n",
                "    print(\"\\nüî• Limpiando cach√© de PyTorch...\")\n",
                "    shutil.rmtree(torch_cache)\n",
                "    print(\"‚úÖ Cach√© de PyTorch eliminado\")\n",
                "else:\n",
                "    print(\"‚ÑπÔ∏è  No hay cach√© de PyTorch\")\n",
                "\n",
                "# Limpiar VRAM si est√° disponible\n",
                "print(\"\\nüíæ Limpiando VRAM...\")\n",
                "try:\n",
                "    import torch\n",
                "    if torch.cuda.is_available():\n",
                "        torch.cuda.empty_cache()\n",
                "        print(\"‚úÖ VRAM limpiada\")\n",
                "    else:\n",
                "        print(\"‚ÑπÔ∏è  GPU no disponible\")\n",
                "except:\n",
                "    print(\"‚ö†Ô∏è  No se pudo limpiar VRAM\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ LIMPIEZA COMPLETADA\")\n",
                "print(\"=\"*60)\n",
                "print(\"\\nüí° Ahora EJECUTA la celda 1.2 para descargar los modelos de nuevo\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "run",
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title üöÄ 2. Ejecutar Servidor Backend\n",
                "\n",
                "import os\n",
                "import time\n",
                "import threading\n",
                "from pathlib import Path\n",
                "from pyngrok import ngrok\n",
                "\n",
                "# Token de ngrok (obt√©n el tuyo en https://dashboard.ngrok.com)\n",
                "AUTH_TOKEN = \"2yHQiBeYhFdbJSiK31054jtsKkw_54yvtD5Cs9mK2yhFgQ2j\" #@param {type:\"string\"}\n",
                "PORT = 5000 #@param {type:\"integer\"}\n",
                "\n",
                "# Validar directorio\n",
                "BACKEND_DIR = Path(\"/content/FoadsIA/backend\")\n",
                "if not BACKEND_DIR.exists():\n",
                "    raise Exception(f\"‚ùå Directorio backend no encontrado: {BACKEND_DIR}\")\n",
                "\n",
                "os.chdir(BACKEND_DIR)\n",
                "print(f\"üìÅ Directorio actual: {os.getcwd()}\\n\")\n",
                "\n",
                "# Verificar app.py\n",
                "if not (BACKEND_DIR / \"app.py\").exists():\n",
                "    raise Exception(\"‚ùå No se encontr√≥ app.py en el directorio backend\")\n",
                "\n",
                "# Configurar ngrok\n",
                "if AUTH_TOKEN:\n",
                "    try:\n",
                "        print(\"üîê Configurando autenticaci√≥n de ngrok...\")\n",
                "        ngrok.set_auth_token(AUTH_TOKEN)\n",
                "        print(\"‚úÖ Token configurado\\n\")\n",
                "    except Exception as e:\n",
                "        print(f\"‚ö†Ô∏è  Error al configurar token: {e}\\n\")\n",
                "else:\n",
                "    print(\"‚ö†Ô∏è  No se proporcion√≥ token de ngrok - la sesi√≥n ser√° limitada\\n\")\n",
                "\n",
                "# Limpiar t√∫neles existentes\n",
                "print(\"üßπ Limpiando t√∫neles anteriores...\")\n",
                "try:\n",
                "    ngrok.kill()\n",
                "    time.sleep(1)\n",
                "    print(\"‚úÖ T√∫neles limpiados\\n\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ÑπÔ∏è  No hab√≠a t√∫neles previos\\n\")\n",
                "\n",
                "# Crear t√∫nel\n",
                "print(f\"üåê Creando t√∫nel p√∫blico en puerto {PORT}...\")\n",
                "try:\n",
                "    tunnel = ngrok.connect(PORT, bind_tls=True)\n",
                "    public_url = tunnel.public_url\n",
                "    \n",
                "    print(\"\\n\" + \"=\"*70)\n",
                "    print(\"üéâ SERVIDOR P√öBLICO ACTIVO\")\n",
                "    print(\"=\"*70)\n",
                "    print(f\"\\nüì° URL del Backend: {public_url}\")\n",
                "    print(f\"\\nüí° Usa esta URL en tu frontend (Settings) para conectarte\")\n",
                "    print(f\"\\n‚ö†Ô∏è  IMPORTANTE: Este t√∫nel se cerrar√° si detienes esta celda\")\n",
                "    print(\"=\"*70 + \"\\n\")\n",
                "    \n",
                "    # Mostrar t√∫neles activos\n",
                "    tunnels = ngrok.get_tunnels()\n",
                "    if tunnels:\n",
                "        print(\"üìã T√∫neles activos:\")\n",
                "        for t in tunnels:\n",
                "            print(f\"   ‚Ä¢ {t.public_url} -> {t.config['addr']}\")\n",
                "        print()\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Error al crear t√∫nel: {e}\")\n",
                "    print(\"\\nüí° Soluciones posibles:\")\n",
                "    print(\"   1. Verifica tu token de ngrok\")\n",
                "    print(\"   2. Revisa tu conexi√≥n a internet\")\n",
                "    print(\"   3. Intenta ejecutar la celda nuevamente\")\n",
                "    raise\n",
                "\n",
                "# Iniciar servidor Flask\n",
                "print(\"üöÄ Iniciando servidor Flask...\\n\")\n",
                "print(\"=\"*70)\n",
                "print(\"üìù LOGS DEL SERVIDOR (Ctrl+C para detener):\")\n",
                "print(\"=\"*70 + \"\\n\")\n",
                "\n",
                "try:\n",
                "    # Ejecutar app.py\n",
                "    !python app.py\n",
                "    \n",
                "except KeyboardInterrupt:\n",
                "    print(\"\\n\\n‚ö†Ô∏è  Servidor detenido por el usuario\")\n",
                "    ngrok.kill()\n",
                "    print(\"‚úÖ T√∫neles cerrados\")\n",
                "    \n",
                "except Exception as e:\n",
                "    print(f\"\\n\\n‚ùå Error al ejecutar servidor: {e}\")\n",
                "    ngrok.kill()\n",
                "    raise\n",
                "\n",
                "finally:\n",
                "    print(\"\\nüîö Limpieza completada\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "debug",
                "cellView": "form"
            },
            "outputs": [],
            "source": [
                "# @title üîç 3. Diagn√≥stico y Pruebas (Opcional)\n",
                "\n",
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "\n",
                "print(\"üîç DIAGN√ìSTICO DEL SISTEMA\\n\")\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Informaci√≥n del sistema\n",
                "print(\"\\nüìä Sistema Operativo:\")\n",
                "!uname -a\n",
                "\n",
                "print(\"\\nüêç Versi√≥n de Python:\")\n",
                "print(f\"   {sys.version}\")\n",
                "\n",
                "print(\"\\nüì¶ Paquetes principales instalados:\")\n",
                "packages = [\"torch\", \"diffusers\", \"flask\", \"flask-socketio\", \"pyngrok\", \"opencv-python\"]\n",
                "for pkg in packages:\n",
                "    try:\n",
                "        result = !pip show {pkg} | grep Version\n",
                "        if result:\n",
                "            print(f\"   ‚úÖ {pkg}: {result[0].split(':')[1].strip()}\")\n",
                "        else:\n",
                "            print(f\"   ‚ùå {pkg}: No instalado\")\n",
                "    except:\n",
                "        print(f\"   ‚ùå {pkg}: Error al verificar\")\n",
                "\n",
                "print(\"\\nüéÆ Estado de GPU:\")\n",
                "try:\n",
                "    import torch\n",
                "    print(f\"   GPU disponible: {torch.cuda.is_available()}\")\n",
                "    if torch.cuda.is_available():\n",
                "        print(f\"   Nombre: {torch.cuda.get_device_name(0)}\")\n",
                "        print(f\"   Memoria total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "        print(f\"   Memoria libre: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_reserved(0)) / 1e9:.1f} GB\")\n",
                "except Exception as e:\n",
                "    print(f\"   ‚ùå Error: {e}\")\n",
                "\n",
                "print(\"\\nüíæ Estado del cach√©:\")\n",
                "hf_cache = Path.home() / \".cache\" / \"huggingface\" / \"hub\"\n",
                "if hf_cache.exists():\n",
                "    sdxl_cache = hf_cache / \"models--ByteDance--SDXL-Lightning\"\n",
                "    if sdxl_cache.exists():\n",
                "        print(\"   ‚úÖ Modelo SDXL Lightning en cach√©\")\n",
                "    else:\n",
                "        print(\"   ‚ÑπÔ∏è  Modelo SDXL Lightning no descargado\")\n",
                "else:\n",
                "    print(\"   ‚ÑπÔ∏è  No hay cach√© de modelos\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"‚úÖ Diagn√≥stico completado\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "footer"
            },
            "source": [
                "---\n",
                "\n",
                "## üìö Notas Importantes\n",
                "\n",
                "### ‚ö° Modelos\n",
                "- **SDXL Lightning**: 5.14 GB. Se descarga una vez y queda en cach√©.\n",
                "- **Whisper Small**: Para reconocimiento de voz. Peque√±o y r√°pido.\n",
                "\n",
                "### üîß Troubleshooting\n",
                "\n",
                "#### Error: \"ImportError: cannot import name 'StableDiffusionXLPipeline'\"\n",
                "**Soluci√≥n**: El c√≥digo ya est√° actualizado para Diffusers v1.0.0+.\n",
                "\n",
                "#### Descarga Fallida\n",
                "**Soluci√≥n**: Ejecuta la celda **1.2 Descargar Modelos** manualmente.\n",
                "\n",
                "### üí° Generaci√≥n R√°pida\n",
                "- La primera imagen tarda ~40s (carga en RAM)\n",
                "- Las siguientes tardan ~2s üî•\n",
                "\n",
                "---\n",
                "\n",
                "**Desarrollado por FoadsIA Team** üöÄ"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}