# üîß Gu√≠a de Troubleshooting - Error de Generaci√≥n de Im√°genes

**Fecha**: 2025-12-27  
**Error Detectado**: HTTP 500 en `/generate-image`

---

## üìä Estado Actual del Sistema

### ‚úÖ Funcionando Correctamente
- ‚úÖ Servidor Flask activo en puerto 5000
- ‚úÖ Ngrok tunnel funcionando: `https://spriggiest-pluggable-roosevelt.ngrok-free.dev`
- ‚úÖ GPU Status endpoint (200 OK)
- ‚úÖ Magic Prompt endpoint (200 OK)
- ‚úÖ SDXL Lightning carg√°ndose correctamente

### ‚ùå Problema Identificado
- ‚ùå `/generate-image` retorna 500 Internal Server Error
- ‚ùå Error ocurre despu√©s de cargar SDXL Lightning

---

## üîç Posibles Causas del Error

### 1. **Error de VRAM Insuficiente**
**S√≠ntoma**: CUDA Out of Memory
**Soluci√≥n**: El sistema de offloading deber√≠a manejarlo, pero verifica:
```python
# En Colab, ejecuta:
import torch
print(f"VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
print(f"VRAM Usada: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB")
```

### 2. **Error en Carga del Modelo**
**S√≠ntoma**: AttributeError o ModuleNotFoundError
**Soluci√≥n**: Verificar que todos los archivos del modelo se descargaron correctamente

### 3. **Error en el Prompt**
**S√≠ntoma**: ValueError o TypeError
**Soluci√≥n**: Ahora validamos que el prompt no est√© vac√≠o

### 4. **Error de Tipo de Datos**
**S√≠ntoma**: RuntimeError con tensores
**Soluci√≥n**: Verificar que torch.float16 sea compatible con la GPU

---

## üõ†Ô∏è Mejoras Implementadas

### Cambios en `app.py`

1. **Logging Detallado**:
   ```python
   print(f"[*] Generating image for prompt: {prompt[:50]}...")
   print(f"[*] Running SDXL inference...")
   print(f"[*] Encoding image to base64...")
   print(f"[‚úì] Image generated successfully")
   ```

2. **Validaci√≥n de Prompt**:
   ```python
   if not prompt:
       return jsonify({"status": "error", "message": "Prompt vac√≠o"}), 400
   ```

3. **Traceback Completo**:
   ```python
   except Exception as e:
       error_trace = traceback.format_exc()
       print(f"[!] Image Generation Error: {str(e)}")
       print(f"[!] Traceback:\n{error_trace}")
   ```

4. **Tipo de Error en Response**:
   ```python
   return jsonify({
       "status": "error", 
       "message": str(e),
       "type": type(e).__name__  # Nuevo
   }), 500
   ```

---

## üìù Pasos para Diagnosticar

### Paso 1: Actualizar el C√≥digo en Colab

```bash
# En Colab, ejecuta:
%cd /content/FoadsIA
!git pull origin main
```

### Paso 2: Reiniciar el Servidor

```bash
# Detener el servidor actual (Ctrl+C en Colab)
# Luego ejecutar:
%cd /content/FoadsIA/backend
!python app.py
```

### Paso 3: Intentar Generar Imagen Nuevamente

Desde el frontend, intenta generar una imagen y observa los logs en Colab.

### Paso 4: Revisar los Logs

Busca en los logs de Colab:
- `[*] Generating image for prompt: ...` ‚Üê Confirma que lleg√≥ el request
- `[*] Running SDXL inference...` ‚Üê Confirma que el modelo se carg√≥
- `[!] Image Generation Error: ...` ‚Üê Muestra el error espec√≠fico
- `[!] Traceback:` ‚Üê Muestra el stack trace completo

---

## üö® Errores Comunes y Soluciones

### Error 1: "CUDA out of memory"
```python
# Soluci√≥n: Limpiar cach√© manualmente
import torch
torch.cuda.empty_cache()

# Luego reintentar
```

### Error 2: "No module named 'diffusers'"
```bash
# Soluci√≥n: Reinstalar dependencias
!pip install -r requirements.txt
```

### Error 3: "AttributeError: 'NoneType' object has no attribute 'to'"
```python
# Soluci√≥n: El modelo no se carg√≥ correctamente
# Verificar que los archivos se descargaron:
!ls -lh ~/.cache/huggingface/hub/
```

### Error 4: "RuntimeError: Expected all tensors to be on the same device"
```python
# Soluci√≥n: Problema con offloading
# Modificar load_sdxl_model() para forzar todo a CUDA:
pipe_image.to("cuda")
pipe_image.unet.to("cuda")
pipe_image.vae.to("cuda")
pipe_image.text_encoder.to("cuda")
```

---

## üî¨ Test Manual del Endpoint

### Desde Python (en Colab):

```python
import requests
import json

url = "http://localhost:5000/generate-image"
payload = {
    "prompt": "a beautiful sunset over mountains"
}

response = requests.post(url, json=payload)
print(f"Status: {response.status_code}")
print(f"Response: {response.json()}")
```

### Desde cURL (en Colab):

```bash
curl -X POST http://localhost:5000/generate-image \
  -H "Content-Type: application/json" \
  -d '{"prompt": "a cat"}'
```

---

## üìä Checklist de Verificaci√≥n

Antes de reportar el error, verifica:

- [ ] GPU est√° disponible (`torch.cuda.is_available()` = True)
- [ ] VRAM suficiente (>8GB libres)
- [ ] Modelo SDXL descargado completamente
- [ ] Dependencias instaladas correctamente
- [ ] C√≥digo actualizado con los cambios recientes
- [ ] Logs muestran el error espec√≠fico

---

## üéØ Pr√≥ximos Pasos

1. **Commit y Push** los cambios:
   ```bash
   git add backend/app.py
   git commit -m "fix: Add detailed logging to image generation endpoint"
   git push origin main
   ```

2. **Actualizar Colab** y reiniciar servidor

3. **Intentar generar imagen** y capturar el error espec√≠fico

4. **Reportar** el error completo con el traceback

---

## üí° Alternativas si el Error Persiste

### Opci√≥n 1: Usar Modelo M√°s Ligero
```python
# Cambiar a Stable Diffusion 1.5 (m√°s ligero)
base = "runwayml/stable-diffusion-v1-5"
# Requiere ~4GB VRAM vs ~6GB de SDXL
```

### Opci√≥n 2: Reducir Resoluci√≥n
```python
# Agregar par√°metros de tama√±o
image = pipe(
    prompt, 
    num_inference_steps=4, 
    guidance_scale=0,
    height=512,  # Reducir de 1024
    width=512    # Reducir de 1024
).images[0]
```

### Opci√≥n 3: Usar CPU Temporalmente
```python
# Para debugging, cargar modelo en CPU
pipe_image.to("cpu")
# Ser√° MUY lento pero permitir√° identificar si es problema de VRAM
```

---

**Documentado por**: Antigravity AI  
**Fecha**: 2025-12-27  
**Versi√≥n**: 1.0
